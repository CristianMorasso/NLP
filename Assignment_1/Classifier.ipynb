{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGMENT 1\n",
    "## English doc classifier\n",
    "\n",
    "This software classifies documents in English or Not English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS \n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import nltk \n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import europarl_raw\n",
    "from nltk.corpus import udhr\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DOWNLOAD DOCUMENTS \n",
    "nltk.download('punkt')\n",
    "nltk.download(\"europarl_raw\")\n",
    "nltk.download(\"udhr\")\n",
    "nltk.download(\"gutenberg\")\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object creation\n",
    "st = PorterStemmer() \n",
    "#st = LancasterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline function\n",
    "The main purpose of this function is parse the documents, in order to remove useless part of words (with the pipeline), remove most frequent words (stopwords) and select the usefull words for the features.\n",
    "\n",
    "Pipeline that process the data following this technique:\n",
    "- Tokenization: Divide docs in single wards to be processed\n",
    "- Stopwords eliminations: elimination of the first n (*stopwards* parameter) most cummon wards\n",
    "- Stemming: process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words \n",
    "- Lemmatization: process of reducing a word to its lemma\n",
    "\n",
    "This fun returns also a list of docs processed and labelled \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data, stopwords = 10, limit =2000):\n",
    "    \"\"\"\n",
    "        Given a list of docs returns them processed by the pipeline and labelled\n",
    "        IN:\\n\n",
    "        text:   list of documents\\n\n",
    "        stopwords:  number of words to ignore\\n\n",
    "        limit: numer of features we keep\n",
    "        OUT:\\n\n",
    "        topWords:    list of topwords ordered by most frequent\n",
    "        DataProcessed:  documents with label \n",
    "    \"\"\"\n",
    "    parole = 0\n",
    "    dataProcessed = [0 for _ in range(len(data))]\n",
    "    fdist = FreqDist()\n",
    "    for i, (doc, l)in enumerate(tqdm(data)):\n",
    "        temp = ([], l)\n",
    "        \n",
    "        #TOKENIZATION \n",
    "        #tokenization doc into words\n",
    "        words = word_tokenize(doc)          \n",
    "        for word in words:\n",
    "            if word not in punctuation and not word.isdecimal():\n",
    "                parole +=1\n",
    "                #Stemming \n",
    "                stemmed= st.stem(word)\n",
    "                #lemmatization\n",
    "                lemmatized= wnl.lemmatize(stemmed) \n",
    "                #counting words elaborated    \n",
    "                fdist[lemmatized] += 1\n",
    "                temp[0].append(lemmatized) \n",
    "\n",
    "        dataProcessed[i]= temp\n",
    "    print(\"Words cardinality: \",parole)\n",
    "    return list(fdist)[stopwords:limit], dataProcessed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features_estractor1 fun\n",
    "\n",
    "Create the features set readable for the NaiveBayes Classifier from a document and top_Words.\n",
    "\n",
    "In particular a dictionary where for each top_Words (tW extract before) there is a boolean value if it is in that document (d) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_estractor1(d, tW):\n",
    "    \"\"\"\n",
    "        Return a dictionary with all words in the tW and a presence value in the document d\\n\n",
    "        IN:\\n\n",
    "        d:  Document to look if the word is in it\\n \n",
    "        tW: set of Words to check (top_Words)\\n\n",
    "        OUT:\\n\n",
    "        dict:   dictionary with {word: presenceValue(bool)}\n",
    "    \"\"\"\n",
    "    ds = set(d)\n",
    "    features = {}\n",
    "    for w in tW:\n",
    "        features[f'contains({w})'] = (w in ds)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "- Var setup (20 docs for English, 30 Non English)\n",
    "- texts load    (English, French, Danish, Finnish)\n",
    "- labelling tests \n",
    "- Data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = 10\n",
    "nLen = 3\n",
    "h_ids = 10  #math.floor(fids/(nLen-1))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "tests = []\n",
    "#prendo fids documenti per ogni lingua\n",
    "#europarl docs\n",
    "en = europarl_raw.english.fileids()[:fids]\n",
    "fr = europarl_raw.french.fileids()[:h_ids] \n",
    "dan = europarl_raw.danish.fileids()[:h_ids]\n",
    "fin = europarl_raw.finnish.fileids()[:h_ids]\n",
    "\n",
    "#gutenberg eng docs \n",
    "gutENGberg_ids = gutenberg.fileids()[:fids]\n",
    "\n",
    "\n",
    "\n",
    "#list of tuples with docs and label\n",
    "#E english N_E not english\n",
    "for ids in gutENGberg_ids:\n",
    "    data.append((gutenberg.raw(ids), \"E\"))\n",
    "for i in range(fids):\n",
    "    data.append((europarl_raw.english.raw(en[i]), \"E\"))\n",
    "for i in range(h_ids):\n",
    "    data.append((europarl_raw.french.raw(fr[i]), \"N_E\"))\n",
    "    data.append((europarl_raw.danish.raw(dan[i]), \"N_E\"))\n",
    "    data.append((europarl_raw.finnish.raw(fin[i]), \"N_E\"))\n",
    "\n",
    "\n",
    "#data shuffle\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:53<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words cardinality:  3403340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#document process \n",
    "#removing the first 10000 words as stopwords and taking the next 10000 words as features \n",
    "topWords, dataProcessed = pipeline(data, stopwords=10000, limit = 15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of:\n",
    "-   Features set\n",
    "-   Training set    .7 Features Set\n",
    "-   Testing set     .3 Features Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 166.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#features creation\n",
    "featuresets = [(features_estractor1(d,topWords),l) for (d,l) in tqdm(dataProcessed)]\n",
    "sep = math.floor(len(featuresets) * 0.7 )\n",
    "train_set, test_set = featuresets[:sep], featuresets[sep:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER LEARNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "refsets =  collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i,(feats,label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    result = classifier.classify(feats)\n",
    "    testsets[result].add(i)\n",
    "    labels.append(label)\n",
    "    tests.append(result)\n",
    "cm = ConfusionMatrix(labels, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Metrics\n",
    "\n",
    "- Docs Labels \n",
    "- Confusion Matrix (*N_E* Not English, *E* English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English docs: 20\n",
      "Not English docs: 30\n",
      "\n",
      "Confusion Matrix:\n",
      "    |      N        |\n",
      "    |      _        |\n",
      "    |      E      E |\n",
      "----+---------------+\n",
      "N_E | <46.7%>  6.7% |\n",
      "  E |      . <46.7%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"English docs: {fids+len(gutENGberg_ids)}\\nNot English docs: {len(data)-(fids+len(gutENGberg_ids))}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333\n",
      "Precision: 0.875\n",
      "Recall: 1.0\n",
      "F-Measure: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",round(nltk.classify.accuracy(classifier, test_set),3)) \n",
    "print( 'Precision:', round(precision(refsets['E'], testsets['E']),3) )\n",
    "print( 'Recall:', round(recall(refsets['E'], testsets['E']),3) )\n",
    "print(\"F-Measure:\", round(f_measure(refsets['E'], testsets['E']),3))\n",
    "#print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Morst informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "       contains(permiss) = True                E : N_E    =      8.2 : 1.0\n",
      "      contains(abstract) = True                E : N_E    =      7.1 : 1.0\n",
      "       contains(appetit) = True                E : N_E    =      7.1 : 1.0\n",
      "        contains(cultiv) = True                E : N_E    =      7.1 : 1.0\n",
      "      contains(diplomat) = True                E : N_E    =      7.1 : 1.0\n",
      "         contains(omiss) = True                E : N_E    =      7.1 : 1.0\n",
      "       contains(reassur) = True                E : N_E    =      7.1 : 1.0\n",
      "        contains(absorb) = True                E : N_E    =      6.0 : 1.0\n",
      "      contains(horizont) = True                E : N_E    =      6.0 : 1.0\n",
      "        contains(infect) = True                E : N_E    =      6.0 : 1.0\n",
      "    contains(leadership) = True                E : N_E    =      6.0 : 1.0\n",
      "        contains(outrag) = True                E : N_E    =      6.0 : 1.0\n",
      "      contains(platform) = True                E : N_E    =      6.0 : 1.0\n",
      "         contains(timid) = True                E : N_E    =      6.0 : 1.0\n",
      "      contains(unilater) = True                E : N_E    =      6.0 : 1.0\n",
      "       contains(audienc) = True                E : N_E    =      4.9 : 1.0\n",
      "        contains(barbar) = True                E : N_E    =      4.9 : 1.0\n",
      "         contains(blank) = True                E : N_E    =      4.9 : 1.0\n",
      "          contains(chat) = True                E : N_E    =      4.9 : 1.0\n",
      "      contains(district) = True                E : N_E    =      4.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results\n",
    "\n",
    "This classifier classifies documents as *English* or *Not English*, this implies that the document needs to be enoght big to be classified.\n",
    "\n",
    "\"It works good because the task is easy\".\n",
    "\n",
    "Talking about metrics we used a confusion matrix to evaluate the model and we can see that all of them are high this is due to the big difference between the languages used in the big corpus used and the small dataset (50 docs) (overfit).\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
