{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGMENT 1\n",
    "## English doc classifier\n",
    "\n",
    "This software classifies documents in English or Not English.\n",
    "\n",
    "Note: the doc should have the same size or bigger then the learning docs.\n",
    "\n",
    "##  STRUCTURE\n",
    "\n",
    "-   *Data Fetching*\n",
    "-   *Pipeline*\n",
    "-   *Feature Extraction*\n",
    "-   *Traning the Model*\n",
    "-   *Results & Conclusion*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS \n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import nltk \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import europarl_raw\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\sonia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DOWNLOAD DOCUMENTS \n",
    "nltk.download('punkt')\n",
    "nltk.download(\"europarl_raw\")\n",
    "nltk.download(\"udhr\")\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object creation\n",
    "st = PorterStemmer() \n",
    "#st = LancasterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline function\n",
    "The main purpose of this function is parse the documents, in order to remove useless part of words (with the pipeline), remove most frequent words (stopwords) and select the usefull words for the features.\n",
    "\n",
    "Pipeline that process the data following this technique:\n",
    "- Tokenization: Divide docs in single wards to be processed\n",
    "- Stopwords eliminations: elimination of the first n (*stopwards* parameter) most cummon wards\n",
    "- Stemming: process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words \n",
    "- Lemmatization: process of reducing a word to its lemma\n",
    "\n",
    "This fun returns also a list of docs processed and labelled \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data, stopwords = 10, limit =2000):\n",
    "    \"\"\"\n",
    "        Given a list of docs returns them processed by the pipeline and labelled\n",
    "        IN:\\n\n",
    "        text:   list of documents\\n\n",
    "        stopwords:  number of words to ignore\\n\n",
    "        limit: numer of features we keep\n",
    "        OUT:\\n\n",
    "        topWords:    list of topwords ordered by most frequent\n",
    "        DataProcessed:  documents with label \n",
    "    \"\"\"\n",
    "    parole = 0\n",
    "    dataProcessed = [0 for _ in range(len(data))]\n",
    "    fdist = FreqDist()\n",
    "    for i, (doc, l)in enumerate(tqdm(data)):\n",
    "        temp = ([], l)\n",
    "        \n",
    "        #TOKENIZATION \n",
    "        #tokenization doc into words\n",
    "        words = word_tokenize(doc)          \n",
    "        for word in words:\n",
    "            if word not in punctuation and not word.isdecimal():\n",
    "                parole +=1\n",
    "                #Stemming \n",
    "                stemmed= st.stem(word)\n",
    "                #lemmatization\n",
    "                lemmatized= wnl.lemmatize(stemmed) \n",
    "                #counting words elaborated    \n",
    "                fdist[lemmatized] += 1\n",
    "                temp[0].append(lemmatized) \n",
    "\n",
    "        dataProcessed[i]= temp\n",
    "        \n",
    "    print(\"Words cardinality: \",parole, \"FDist Cardinality: \", len(fdist))\n",
    "    return list(fdist)[stopwords:limit], dataProcessed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features_estractor1 fun\n",
    "\n",
    "Create the features set readable for the NaiveBayes Classifier from a document and top_Words.\n",
    "\n",
    "In particular a dictionary where for each top_Words (tW extract before) there is a boolean value if it is in that document (d) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_estractor1(d, tW):\n",
    "    \"\"\"\n",
    "        Return a dictionary with all words in the tW and a presence value in the document d\\n\n",
    "        IN:\\n\n",
    "        d:  Document to look if the word is in it\\n \n",
    "        tW: set of Words to check (top_Words)\\n\n",
    "        OUT:\\n\n",
    "        dict:   dictionary with {word: presenceValue(bool)}\n",
    "    \"\"\"\n",
    "    ds = set(d)\n",
    "    features = {}\n",
    "    for w in tW:\n",
    "        features[f'contains({w})'] = (w in ds)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "### Data fetching\n",
    "- Var setup (20 docs for English, 30 Non English)\n",
    "- texts load    (English, French, Danish, Finnish)\n",
    "- labelling tests \n",
    "- Data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = 10\n",
    "nLen = 3\n",
    "h_ids = 10  #math.floor(fids/(nLen-1))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "tests = []\n",
    "\n",
    "#europarl docs\n",
    "en = europarl_raw.english.fileids()[:fids]\n",
    "fr = europarl_raw.french.fileids()[:h_ids] \n",
    "dan = europarl_raw.danish.fileids()[:h_ids]\n",
    "fin = europarl_raw.finnish.fileids()[:h_ids]\n",
    "\n",
    "#gutenberg eng docs \n",
    "gutENGberg_ids = gutenberg.fileids()[:fids]\n",
    "\n",
    "\n",
    "\n",
    "#list of tuples with docs and label\n",
    "#E english N_E not english\n",
    "for ids in gutENGberg_ids:\n",
    "    data.append((gutenberg.raw(ids), \"E\"))\n",
    "for i in range(fids):\n",
    "    data.append((europarl_raw.english.raw(en[i]), \"E\"))\n",
    "for i in range(h_ids):\n",
    "    data.append((europarl_raw.french.raw(fr[i]), \"N_E\"))\n",
    "    data.append((europarl_raw.danish.raw(dan[i]), \"N_E\"))\n",
    "    data.append((europarl_raw.finnish.raw(fin[i]), \"N_E\"))\n",
    "\n",
    "\n",
    "#data shuffle\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Processing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:31<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words cardinality:  3403340 FDist Cardinality:  107899\n"
     ]
    }
   ],
   "source": [
    "#document process \n",
    "#removing the first 10000 words as stopwords and taking the next 5000 words as features \n",
    "topWords, dataProcessed = pipeline(data, stopwords=10000, limit = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "#### Creation of:\n",
    "-   Features set\n",
    "-   Training set    .7 Features Set\n",
    "-   Testing set     .3 Features Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 70.36it/s]\n"
     ]
    }
   ],
   "source": [
    "#features creation\n",
    "featuresets = [(features_estractor1(d,topWords),l) for (d,l) in tqdm(dataProcessed)]\n",
    "sep = math.floor(len(featuresets) * 0.7 )\n",
    "train_set, test_set = featuresets[:sep], featuresets[sep:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "refsets =  collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i,(feats,label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    result = classifier.classify(feats)\n",
    "    testsets[result].add(i)\n",
    "    labels.append(label)\n",
    "    tests.append(result)\n",
    "cm = ConfusionMatrix(labels, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Metrics\n",
    "\n",
    "- Docs Labels \n",
    "- Confusion Matrix (*N_E* Not English, *E* English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English docs: 20\n",
      "Not English docs: 30\n",
      "Avarange words for document:  68067\n",
      "\n",
      "Confusion Matrix:\n",
      "    |      N        |\n",
      "    |      _        |\n",
      "    |      E      E |\n",
      "----+---------------+\n",
      "N_E | <66.7%>     . |\n",
      "  E |      . <33.3%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"English docs: {fids+len(gutENGberg_ids)}\\nNot English docs: {len(data)-(fids+len(gutENGberg_ids))}\")\n",
    "print(\"Avarange words for document: \", round(np.mean([len(d[0]) for d in dataProcessed],0)))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",round(nltk.classify.accuracy(classifier, test_set),3)) \n",
    "print( 'Precision:', round(precision(refsets['E'], testsets['E']),3) )\n",
    "print( 'Recall:', round(recall(refsets['E'], testsets['E']),3) )\n",
    "print(\"F-Score:\", round(f_measure(refsets['E'], testsets['E']),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Morst informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "      contains(diplomat) = True                E : N_E    =      6.6 : 1.0\n",
      "          contains(sale) = True                E : N_E    =      6.6 : 1.0\n",
      "         contains(timid) = True                E : N_E    =      6.6 : 1.0\n",
      "        contains(absorb) = True                E : N_E    =      5.7 : 1.0\n",
      "      contains(abstract) = True                E : N_E    =      5.7 : 1.0\n",
      "       contains(appetit) = True                E : N_E    =      5.7 : 1.0\n",
      "         contains(blank) = True                E : N_E    =      5.7 : 1.0\n",
      "      contains(fluctuat) = True                E : N_E    =      5.7 : 1.0\n",
      "       contains(illumin) = True                E : N_E    =      5.7 : 1.0\n",
      "        contains(obstin) = True                E : N_E    =      5.7 : 1.0\n",
      "       contains(permiss) = True                E : N_E    =      5.7 : 1.0\n",
      "      contains(platform) = True                E : N_E    =      5.7 : 1.0\n",
      "       contains(redress) = True                E : N_E    =      5.7 : 1.0\n",
      "       contains(repress) = True                E : N_E    =      5.7 : 1.0\n",
      "       contains(scrupul) = True                E : N_E    =      5.7 : 1.0\n",
      "      contains(undermin) = True                E : N_E    =      5.7 : 1.0\n",
      "           contains(nut) = True                E : N_E    =      5.0 : 1.0\n",
      "        contains(barbar) = True                E : N_E    =      4.8 : 1.0\n",
      "        contains(infect) = True                E : N_E    =      4.8 : 1.0\n",
      "    contains(leadership) = True                E : N_E    =      4.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results and Conclusions\n",
    "\n",
    "This classifier classifies documents as *English* or *Not English*, this implies that the document needs to be enoght big to be classified.\n",
    "\n",
    "\"It works good because the task is easy\".\n",
    "\n",
    "Talking about metrics we used a confusion matrix to evaluate the model and we can see that all of them are high this is due to the big difference between the languages used in the big corpus used and the small dataset (50 docs) (overfit).\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "Discuss:\n",
    "1.  size of the corpus, size of the split training and test sets\n",
    "2.  performance indicators employed and their nature\n",
    "3.  employability of the classifier as a Probabilistic Language Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  In this case the corpus size consist in 50 docs, 20 english and 30 not english (over 65000 words on avarage), that's leads to have high model performances because every doc is big. Then we are working with small docs set thats causes overfitting problems.</br>\n",
    "Talking about training and test sets we divided the feature sets .7 for train and .3 for test, this division is biased towards training in order cover the overfitting problem. \n",
    "2.  The metrics used are:\n",
    "    -   **Precision** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "    -   **Recall** (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class *E*\n",
    "    -   **F1 score** - F1 Score is the weighted average of Precision and Recall, it is more robust then *accuracy*\n",
    "3.  This classifier can classifies \"large\" documents like long speeches and documents larger then 50000 words.</br>\n",
    "Maybe the topic could be a problem but with over 60k words and 10k features it should cover a good part of the topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
